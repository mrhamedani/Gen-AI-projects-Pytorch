{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQEMd8BNnC0ZfisMbijj53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/Gen-AI-projects-Pytorch/blob/main/9_TextGenerationLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jgleZ0svM3Q",
        "outputId": "dffacc95-44fb-402f-c125-85f8cb49b3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'i', ',', ' ', 't', 'h', 'e', 'r', 'e', '!']\n"
          ]
        }
      ],
      "source": [
        " # exercise 1\n",
        "text=\"Hi, there!\"\n",
        "tokens=list(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 2: word tokenization\n",
        "text=\"It is unbelievably good!\"\n",
        "text=text.replace(\"!\",\"\")\n",
        "tokens=text.split(\" \")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebPKREENxBY3",
        "outputId": "9a469332-16fc-42f0-ad27-6d5278e40215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'is', 'unbelievably', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 3 :\n",
        "text=\"Hi, there!\"\n",
        "for x in list(\",!\"):\n",
        "    text=text.replace(f\"{x}\",f\" {x}\")\n",
        "tokens=text.split(\" \")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8N_DVlpxJYu",
        "outputId": "104ae574-3c9e-4ea2-ea92-86f14ff7a13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', ',', 'there', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & clean up the text"
      ],
      "metadata": {
        "id": "431Sks85yHpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O anna.txt \"https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/anna.txt?ref_type=heads&inline=false\" # text of anna karenina"
      ],
      "metadata": {
        "id": "rVqTxcr8xmDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./anna.txt\",\"r\") as f:\n",
        "    text=f.read()\n",
        "words=text.split(\" \")\n",
        "print(words[:20])\n",
        "\n",
        "print(set(text.lower())) # unique characters in the text by set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awHgakl9xNo_",
        "outputId": "86ad1f51-26af-4339-ae32-56afb25873e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\"]\n",
            "{'c', 'g', 'z', '\\n', \"'\", 'i', ')', 'e', '`', 's', 'm', 'v', '_', '9', 'r', '5', '3', 'd', '\"', 'k', ';', 'h', 'u', '?', ',', ':', 'l', 'p', 'y', 'f', 'b', '-', 'j', '6', ' ', 'o', 'w', '(', '7', 't', '.', '4', '8', '1', '!', 'n', 'a', '0', '2', 'x', 'q'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text=text.lower().replace(\"\\n\", \" \")\n",
        "clean_text=clean_text.replace(\"-\", \" \")\n",
        "for x in \",.:;?!$()/_&%*@'`\":\n",
        "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
        "clean_text=clean_text.replace('\"', ' \" ')\n",
        "text=clean_text.split()"
      ],
      "metadata": {
        "id": "_axkEgBUyBkz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(text)\n",
        "\n",
        "words=sorted(word_counts, key=word_counts.get, reverse=True) # get unique words\n",
        "print(words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDeygMNxyu9C",
        "outputId": "a9424041-f60e-46fd-9188-4f4da6d9a46d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here we will make two dictionaries where the key values ​​are in the same place We will use it further"
      ],
      "metadata": {
        "id": "NkSmWAOCzN38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_length =len(text)\n",
        "num_unique_words =len(words)\n",
        "print(f\"the text contains {text_length} words\")\n",
        "print(f\"there are {num_unique_words} unique tokens\")\n",
        "word_to_int={v:k for k,v in enumerate(words)} # k is number of word repetition and ,v is word\n",
        "int_to_word={k:v for k,v in enumerate(words)}\n",
        "print({k:v for k,v in word_to_int.items() if k in words[:10]})\n",
        "print({k:v for k,v in int_to_word.items() if v in words[:10]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kRaZA_cyzsV",
        "outputId": "c084223b-8e77-471e-a026-bb8671903b09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the text contains 437098 words\n",
            "there are 12778 unique tokens\n",
            "{',': 0, '.': 1, 'the': 2, '\"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7, \"'\": 8, 'a': 9}\n",
            "{0: ',', 1: '.', 2: 'the', 3: '\"', 4: 'and', 5: 'to', 6: 'of', 7: 'he', 8: \"'\", 9: 'a'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0:20])\n",
        "wordidx=[word_to_int[w] for w in text]\n",
        "print([word_to_int[w] for w in text[0:20]])  # The number of each word is the number of repetitions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFM7oOrGzJv1",
        "outputId": "d7181ede-8db0-4d81-8d28-1182848235bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
            "[208, 2755, 280, 2981, 83, 31, 2419, 35, 202, 685, 362, 38, 685, 10, 236, 147, 166, 1, 149, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create batches of training data"
      ],
      "metadata": {
        "id": "eshoWUek0fsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "seq_len=100\n",
        "xys=[]\n",
        "for n in range(0, len(wordidx)-seq_len-1):  # 0 : 437098 -100 - 1\n",
        "    x = wordidx[n:n+seq_len]                # x= 0:100   y = 1:101\n",
        "    y = wordidx[n+1:n+seq_len+1]            # x= 1:101   y = 2:102\n",
        "    xys.append((torch.tensor(x),(torch.tensor(y))))"
      ],
      "metadata": {
        "id": "zkgfo1Xz0RDT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "batch_size=32\n",
        "loader = DataLoader(xys, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "x,y=next(iter(loader))\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape,y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvhpkC_o0kQ3",
        "outputId": "0905f278-9192-4df6-e282-dc36db2a4e56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  39,   31,    2,  ...,  688,  142,    7],\n",
            "        [ 156, 5293,    0,  ...,   38,  330,    0],\n",
            "        [   3,   97,    0,  ...,    0, 1774,   34],\n",
            "        ...,\n",
            "        [  16,  156,    9,  ...,  113,    5,  533],\n",
            "        [   3,    4,   31,  ...,   98,    5,   98],\n",
            "        [ 289,   19,   23,  ...,    9,  828,  550]])\n",
            "tensor([[  31,    2, 2727,  ...,  142,    7,    0],\n",
            "        [5293,    0,   16,  ...,  330,    0,    3],\n",
            "        [  97,    0,    4,  ..., 1774,   34,    3],\n",
            "        ...,\n",
            "        [ 156,    9,  489,  ...,    5,  533,   27],\n",
            "        [   4,   31,   25,  ...,    5,   98,    1],\n",
            "        [  19,   23,    1,  ...,  828,  550,    1]])\n",
            "torch.Size([32, 100]) torch.Size([32, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an LSTM model\n",
        "### LSTM uses token and hidden state to create the next token and hidden state\n",
        "The hidden state stores past information at each step, helping the LSTM understand dependencies between words in a sequence. In every step, a new hidden state is generated and passed to the next step. It works alongside the cell state to maintain both short-term and long-term information."
      ],
      "metadata": {
        "id": "smFyu-i7U9vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, input_size=128, n_embed=128,\n",
        "             n_layers=3, drop_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.input_size=input_size\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_embed = n_embed\n",
        "        vocab_size=len(word_to_int)\n",
        "        self.embedding=nn.Embedding(vocab_size,n_embed)\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
        "            hidden_size=self.n_embed,\n",
        "            num_layers=self.n_layers,\n",
        "            dropout=self.drop_prob,batch_first=True)\n",
        "        self.fc = nn.Linear(input_size, vocab_size) # vocab_size is the number of unique words\n",
        "    def forward(self, x, hc):\n",
        "        embed=self.embedding(x)\n",
        "        x, hc = self.lstm(embed, hc) #hc is tuple of \"hidden state\" and \"cell state\"\n",
        "        x = self.fc(x)\n",
        "        return x, hc\n",
        "    def init_hidden(self, n_seqs):             #Creates two Tensors (hidden state and cell state) with an initial value of zero.\n",
        "        weight = next(self.parameters()).data       #In order that hc is not affected by previous epochs in each epoch, we set it to zero every time\n",
        "        return (weight.new(self.n_layers,n_seqs, self.n_embed).zero_(), #3*32*128\n",
        "                weight.new(self.n_layers,n_seqs, self.n_embed).zero_())"
      ],
      "metadata": {
        "id": "tI-j3T_Z1u50"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=WordLSTM().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "g1P3LhH5TRqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "weOEAgePUiiL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the LSTM model\n",
        "Because the train takes time, I use the train file"
      ],
      "metadata": {
        "id": "jy8uE2KyVIVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.train()\n",
        "\n",
        "# for epoch in range(50):\n",
        "#     tloss=0\n",
        "#     sh,sc = model.init_hidden(batch_size)\n",
        "#     for i, (x,y) in enumerate(loader):\n",
        "#         if x.shape[0]==batch_size:\n",
        "#             inputs, targets = x.to(device), y.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             output, (sh,sc) = model(inputs, (sh,sc))\n",
        "#             loss = loss_func(output.transpose(1,2),targets)\n",
        "#             sh,sc=sh.detach(),sc.detach()\n",
        "#             loss.backward()\n",
        "#             nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "#             optimizer.step()\n",
        "#             tloss+=loss.item()\n",
        "#         if (i+1)%1000==0:\n",
        "#             print(f\"at epoch {epoch} iteration {i+1}\\\n",
        "#             average loss = {tloss/(i+1)}\")"
      ],
      "metadata": {
        "id": "GBbE9qR0U5wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate text with the trained LSTM model\n",
        "Generate text by predicting the next token.\n",
        "in real we want Continue to generate prpmpt with 200 model tokens"
      ],
      "metadata": {
        "id": "ILX0FlE5VqNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O wordLSTM.pth \"https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/wordLSTM.pth?ref_type=heads&inline=false\"\n",
        "# download the model weights file from the link"
      ],
      "metadata": {
        "id": "Apim5TXoXgYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"wordLSTM.pth\", map_location=device))"
      ],
      "metadata": {
        "id": "9r76fH2kVucM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For example, if our prompt is four words, we want the model to predict the next 196 words so that a total of 200 tokens are answered.We also put a condition that if the number of generated words is more than 100 (for example, to generate the 150th word), choose only the last 100."
      ],
      "metadata": {
        "id": "E_hEC15ASHWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sample(model, prompt, length=200):  # length is the number of tokens to generate by the model\n",
        "    model.eval()\n",
        "    text = prompt.lower().split(' ') # text is a list of words prompt\n",
        "    hc = model.init_hidden(1)      # 3*1*128\n",
        "    length = length - len(text)\n",
        "    for i in range(0, length):\n",
        "        # if the text length is less than seq_len=100, use text to predict\n",
        "        if len(text)<=seq_len:  #seq_len = 100\n",
        "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
        "        # otherwise use the last seq_len=100 tokens to predict\n",
        "        else:\n",
        "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])\n",
        "        inputs = x.to(device)\n",
        "        output, hc = model(inputs, hc)\n",
        "        logits = output[0][-1]\n",
        "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy() # p is the probability normalized of each word\n",
        "        idx = np.random.choice(len(logits), p=p)\n",
        "        text.append(int_to_word[idx])\n",
        "    text=\" \".join(text)\n",
        "    for m in \",.:;?!$()/_&%*@'`\":\n",
        "        text=text.replace(f\" {m}\", f\"{m} \")\n",
        "    text=text.replace('\"  ', '\"')\n",
        "    text=text.replace(\"'  \", \"'\")\n",
        "    text=text.replace('\" ', '\"')\n",
        "    text=text.replace(\"' \", \"'\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "ZtQ82iQsVycq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "print(sample(model, prompt='Anna and the prince'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPhWhHHtV3UH",
        "outputId": "8b84e20f-4d16-48f6-fdb8-e756f9f2d68b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna and the prince did not forget what he had not spoken.  when the softening barrier was not so long as he had talked to his brother,  all the hopelessness of the impression.  \"official tail,  a man who had tried him,  though he had been able to get across his charge and locked close,  and the light round the snow was in the light of the altar villa.  the article in law levin was first more precious than it was to him so that if it was most easy as it would be as the same.  this was now perfectly interested.  when he had got up close out into the sledge,  but it was locked in the light window with their one grass,  and in the band of the leaves of his projects,  and all the same stupid woman,  and really,  and i swung his arms round that thinking of bed.  a little box with the two boys were with the point of a gleam of filling the boy,  noiselessly signed the bottom of his mouth,  and answering them took the red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attached\n",
        "### Here the project is finished and we have reached the goal. Next, we want to check how creativity is added to the model by Temperature and top-K sampling"
      ],
      "metadata": {
        "id": "IaC1DXsO8qr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt, top_k=None, length=200, temperature=1): # this function is the same as sample function but with different parameters:A,B,C\n",
        "    model.eval()\n",
        "    text = prompt.lower().split(' ')\n",
        "    hc = model.init_hidden(1)\n",
        "    length = length - len(text)\n",
        "    for i in range(0, length):\n",
        "        # if the text length is less than seq_len, use text to predict\n",
        "        if len(text)<=seq_len:\n",
        "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
        "        # otherwise use the last seq_len tokens to predict\n",
        "        else:\n",
        "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])\n",
        "        inputs = x.to(device)\n",
        "        output, hc = model(inputs, hc)\n",
        "        logits = output[0][-1]\n",
        "        # scale the logits with the temperature       A\n",
        "        logits = logits/temperature\n",
        "        p = nn.functional.softmax(logits, dim=0).detach().cpu()   # C\n",
        "        if top_k is None:\n",
        "            idx = np.random.choice(len(logits), p=p.numpy())  # C\n",
        "        # top-K sampling            B\n",
        "        else:\n",
        "            ps, tops = p.topk(top_k)\n",
        "            ps=ps/ps.sum()\n",
        "            idx = np.random.choice(tops, p=ps.numpy())        #  C\n",
        "        text.append(int_to_word[idx])\n",
        "    text=\" \".join(text)\n",
        "    for m in \",.:;?!$()/_&%*@'`\":\n",
        "        text=text.replace(f\" {m}\", f\"{m} \")\n",
        "    text=text.replace('\"  ', '\"')\n",
        "    text=text.replace(\"'  \", \"'\")\n",
        "    text=text.replace('\" ', '\"')\n",
        "    text=text.replace(\"' \", \"'\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "VbS_mUXaV72t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "myyyAtifAhO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # next token using default setting\n",
        "prompt=\"I ' m not going to see\"\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "for _ in range(10):\n",
        "    print(generate(model, prompt, top_k=None, length=len(prompt.split(\" \"))+1, temperature=1))"
      ],
      "metadata": {
        "id": "NxTTWfQj9RKf",
        "outputId": "ae2857d4-e238-4911-860d-2a0bb0ff2d6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i'm not going to see you\n",
            "i'm not going to see those\n",
            "i'm not going to see me\n",
            "i'm not going to see you\n",
            "i'm not going to see her\n",
            "i'm not going to see her\n",
            "i'm not going to see the\n",
            "i'm not going to see my\n",
            "i'm not going to see you\n",
            "i'm not going to see me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # next token using conservative predictions\n",
        "prompt=\"I ' m not going to see\"\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "for _ in range(10):\n",
        "    print(generate(model, prompt, top_k=3,\n",
        "         length=len(prompt.split(\" \"))+1, temperature=0.5))"
      ],
      "metadata": {
        "id": "Y7XbjsBm9XMS",
        "outputId": "5b8a2f90-884a-4dbf-f081-1ad2a76d5427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i'm not going to see you\n",
            "i'm not going to see the\n",
            "i'm not going to see her\n",
            "i'm not going to see you\n",
            "i'm not going to see you\n",
            "i'm not going to see you\n",
            "i'm not going to see you\n",
            "i'm not going to see her\n",
            "i'm not going to see you\n",
            "i'm not going to see her\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "print(generate(model, prompt='Anna and the prince',top_k=3,temperature=0.5))"
      ],
      "metadata": {
        "id": "VI1kce7G9cPp",
        "outputId": "3602357d-8b05-4c53-8306-2d9f73906e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna and the prince had no milk.  but,  \"answered levin,  and he stopped.  \"i've been skating to look at you all the harrows,  and i'm glad. . .  \"\"no,  i'm going to the country.  \"\"no,  it's not a nice fellow.  \"\"yes,  sir.  \"\"well,  what do you think about it?  \"\"why,  what's the matter?  \"\"yes,  yes,  \"answered levin,  smiling,  and he went into the hall.  \"yes,  i'll come for him and go away,  \"he said,  looking at the crumpled front of his shirt.  \"i have not come to see him,  \"she said,  and she went out.  \"i'm very glad,  \"she said,  with a slight bow to the ambassador's hand.  \"i'll go to the door.  \"she looked at her watch,  and she did not know what to say\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "print(generate(model, prompt='Anna and the nurse',top_k=10,temperature=0.6))"
      ],
      "metadata": {
        "id": "6Ng19Q1n9gwd",
        "outputId": "60ec9b57-d4f2-4452-e0df-ed225dd16b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna and the nurse was at one who's to see his wife,  who was not in the least set in which there had been stuck on all the dogs.  the party in the village elder,  the landowners divided to the accompaniment of men,  whom the young peasant had been very parents had been born in the most horrible people,  and decided in the summer,  and had a straightforward functionary of that family,  a german,  a german,  a personage of vaska,  koznishev with his rank,  probably an old betrothal clerk and a good humored.  alexey alexandrovitch had gone to bed and attacking his wife in the country.  the countess lidia ivanovna's good natured fellow was the colonel.  anna arkadyevna and went up to her.  \"well,  what is it?  \"\"how nicely is it the truth?  \"she asked,  with a pause.  \"oh,  that's a joke,  and to be sure with the measures to yield that's true,  \"said sergey ivanovitch,  with a subtle smile.  \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next token using creative predictions\n",
        "prompt=\"I ' m not going to see\"\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "for _ in range(10):\n",
        "    print(generate(model, prompt, top_k=None, length=len(prompt.split(\" \"))+1, temperature=2))"
      ],
      "metadata": {
        "id": "bRLar9979lp9",
        "outputId": "69b3408b-9192-4cbb-a1fa-19a6ba40f428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i'm not going to see them\n",
            "i'm not going to see scarlatina\n",
            "i'm not going to see behind\n",
            "i'm not going to see us\n",
            "i'm not going to see it\n",
            "i'm not going to see it\n",
            "i'm not going to see a\n",
            "i'm not going to see misery\n",
            "i'm not going to see another\n",
            "i'm not going to see seryozha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "print(generate(model, prompt='Anna and the prince',top_k=None,temperature=2))"
      ],
      "metadata": {
        "id": "gt927s9Y9toK",
        "outputId": "a1255f7b-d618-4701-b3a7-594321613735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna and the prince took sheaves covered suddenly people.  \"pyotr marya borissovna,  propped mihail though her son will seen how much evening her husband;  if tomorrow she liked great time too.  \"adopted heavens details for it women from this terrible,  admitting this touching all everything ill with flirtation shame consolation altogether:  ivan only all the circle with her honorable carriage in its house dress,  beethoven ashamed had the conversations raised mihailov stay of close i taste work?  \"on new farming show ivan nothing.  hat yesterday if interested understand every hundred of two with six thousand roubles according to women living over a thousand:  snetkov possibly try disagreeable schools with stake old glory mysterious one have people some moral conclusion,  got down and then their wreath.  darya alexandrovna thought inwardly peaceful with varenka out of the listen from and understand presented she was impossible anguish.  simply satisfied with staying after presence came where he pushed up his hand as marya her pretty hands into their quarters.  waltz was about the rider gathered;  sviazhsky further alone have an hand paused riding towards an exquisite\n"
          ]
        }
      ]
    }
  ]
}