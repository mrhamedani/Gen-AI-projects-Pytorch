{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPyERnphW1m9dzqtiVNOj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/Gen-AI-projects-Pytorch/blob/main/9_TextGenerationLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jgleZ0svM3Q",
        "outputId": "dffacc95-44fb-402f-c125-85f8cb49b3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'i', ',', ' ', 't', 'h', 'e', 'r', 'e', '!']\n"
          ]
        }
      ],
      "source": [
        " # exercise 1\n",
        "text=\"Hi, there!\"\n",
        "tokens=list(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 2: word tokenization\n",
        "text=\"It is unbelievably good!\"\n",
        "text=text.replace(\"!\",\"\")\n",
        "tokens=text.split(\" \")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebPKREENxBY3",
        "outputId": "9a469332-16fc-42f0-ad27-6d5278e40215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'is', 'unbelievably', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 3 :\n",
        "text=\"Hi, there!\"\n",
        "for x in list(\",!\"):\n",
        "    text=text.replace(f\"{x}\",f\" {x}\")\n",
        "tokens=text.split(\" \")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8N_DVlpxJYu",
        "outputId": "104ae574-3c9e-4ea2-ea92-86f14ff7a13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', ',', 'there', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & clean up the text"
      ],
      "metadata": {
        "id": "431Sks85yHpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O anna.txt \"https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/anna.txt?ref_type=heads&inline=false\" # text of anna karenina"
      ],
      "metadata": {
        "id": "rVqTxcr8xmDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b06849-92f0-466b-8429-350d2ac9a354"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-03 14:54:00--  https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/anna.txt?ref_type=heads&inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2006029 (1.9M) [application/octet-stream]\n",
            "Saving to: ‘anna.txt’\n",
            "\n",
            "anna.txt            100%[===================>]   1.91M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-02-03 14:54:00 (24.8 MB/s) - ‘anna.txt’ saved [2006029/2006029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./anna.txt\",\"r\") as f:\n",
        "    text=f.read()\n",
        "words=text.split(\" \")\n",
        "print(words[:20])\n",
        "\n",
        "print(set(text.lower())) # unique characters in the text by set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awHgakl9xNo_",
        "outputId": "787e7f33-1a0c-4975-f264-83f720d4f307"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\"]\n",
            "{'z', ' ', '0', 'l', '\\n', '.', 'v', 'a', 's', 't', 'u', '?', 'f', \"'\", 'd', ';', ',', '-', 'c', '5', 'm', 'y', 'k', 'o', 'x', 'p', '9', 'q', '8', '2', '1', '_', 'e', 'h', 'g', 'n', ':', 'b', '(', '4', '!', '\"', '3', '6', '`', 'j', 'w', '7', ')', 'r', 'i'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text=text.lower().replace(\"\\n\", \" \")\n",
        "clean_text=clean_text.replace(\"-\", \" \")\n",
        "for x in \",.:;?!$()/_&%*@'`\":\n",
        "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
        "clean_text=clean_text.replace('\"', ' \" ')\n",
        "text=clean_text.split()"
      ],
      "metadata": {
        "id": "_axkEgBUyBkz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(text)\n",
        "\n",
        "words=sorted(word_counts, key=word_counts.get, reverse=True) # get unique words\n",
        "print(words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDeygMNxyu9C",
        "outputId": "bb6c6725-f2af-4afa-83e0-f9cd24ad690b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here we will make two dictionaries where the key values ​​are in the same place We will use it further"
      ],
      "metadata": {
        "id": "NkSmWAOCzN38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_length =len(text)\n",
        "num_unique_words =len(words)\n",
        "print(f\"the text contains {text_length} words\")\n",
        "print(f\"there are {num_unique_words} unique tokens\")\n",
        "word_to_int={v:k for k,v in enumerate(words)} # k is number of word repetition and ,v is word\n",
        "int_to_word={k:v for k,v in enumerate(words)}\n",
        "print({k:v for k,v in word_to_int.items() if k in words[:10]})\n",
        "print({k:v for k,v in int_to_word.items() if v in words[:10]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kRaZA_cyzsV",
        "outputId": "ecdfdbf1-a181-419d-acd8-2242ad59b30b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the text contains 437098 words\n",
            "there are 12778 unique tokens\n",
            "{',': 0, '.': 1, 'the': 2, '\"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7, \"'\": 8, 'a': 9}\n",
            "{0: ',', 1: '.', 2: 'the', 3: '\"', 4: 'and', 5: 'to', 6: 'of', 7: 'he', 8: \"'\", 9: 'a'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0:20])\n",
        "wordidx=[word_to_int[w] for w in text]\n",
        "print([word_to_int[w] for w in text[0:20]])  # The number of each word is the number of repetitions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFM7oOrGzJv1",
        "outputId": "a0b34724-8fc9-4d50-b605-e77513e61c22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
            "[208, 2755, 280, 2981, 83, 31, 2419, 35, 202, 685, 362, 38, 685, 10, 236, 147, 166, 1, 149, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create batches of training data"
      ],
      "metadata": {
        "id": "eshoWUek0fsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "seq_len=100\n",
        "xys=[]\n",
        "for n in range(0, len(wordidx)-seq_len-1):  # 0 : 437098 -100 - 1\n",
        "    x = wordidx[n:n+seq_len]                # x= 0:100   y = 1:101\n",
        "    y = wordidx[n+1:n+seq_len+1]            # x= 1:101   y = 2:102\n",
        "    xys.append((torch.tensor(x),(torch.tensor(y))))"
      ],
      "metadata": {
        "id": "zkgfo1Xz0RDT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "batch_size=32\n",
        "loader = DataLoader(xys, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "x,y=next(iter(loader))\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape,y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvhpkC_o0kQ3",
        "outputId": "0356aa0e-ef96-448c-d94f-b8e4561e84ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  39,   31,    2,  ...,  688,  142,    7],\n",
            "        [ 156, 5293,    0,  ...,   38,  330,    0],\n",
            "        [   3,   97,    0,  ...,    0, 1774,   34],\n",
            "        ...,\n",
            "        [  16,  156,    9,  ...,  113,    5,  533],\n",
            "        [   3,    4,   31,  ...,   98,    5,   98],\n",
            "        [ 289,   19,   23,  ...,    9,  828,  550]])\n",
            "tensor([[  31,    2, 2727,  ...,  142,    7,    0],\n",
            "        [5293,    0,   16,  ...,  330,    0,    3],\n",
            "        [  97,    0,    4,  ..., 1774,   34,    3],\n",
            "        ...,\n",
            "        [ 156,    9,  489,  ...,    5,  533,   27],\n",
            "        [   4,   31,   25,  ...,    5,   98,    1],\n",
            "        [  19,   23,    1,  ...,  828,  550,    1]])\n",
            "torch.Size([32, 100]) torch.Size([32, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an LSTM model"
      ],
      "metadata": {
        "id": "smFyu-i7U9vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, input_size=128, n_embed=128,\n",
        "             n_layers=3, drop_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.input_size=input_size\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_embed = n_embed\n",
        "        vocab_size=len(word_to_int)\n",
        "        self.embedding=nn.Embedding(vocab_size,n_embed)\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
        "            hidden_size=self.n_embed,\n",
        "            num_layers=self.n_layers,\n",
        "            dropout=self.drop_prob,batch_first=True)\n",
        "        self.fc = nn.Linear(input_size, vocab_size)\n",
        "    def forward(self, x, hc):\n",
        "        embed=self.embedding(x)\n",
        "        x, hc = self.lstm(embed, hc)\n",
        "        x = self.fc(x)\n",
        "        return x, hc\n",
        "    def init_hidden(self, n_seqs):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers,\n",
        "                           n_seqs, self.n_embed).zero_(),\n",
        "                weight.new(self.n_layers,\n",
        "                           n_seqs, self.n_embed).zero_())"
      ],
      "metadata": {
        "id": "tI-j3T_Z1u50"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=WordLSTM().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "g1P3LhH5TRqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6d2ce4-5681-4f98-b637-3ca3da87a50b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (embedding): Embedding(12778, 128)\n",
            "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=128, out_features=12778, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "weOEAgePUiiL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the LSTM model"
      ],
      "metadata": {
        "id": "jy8uE2KyVIVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.train()\n",
        "\n",
        "# for epoch in range(50):\n",
        "#     tloss=0\n",
        "#     sh,sc = model.init_hidden(batch_size)\n",
        "#     for i, (x,y) in enumerate(loader):\n",
        "#         if x.shape[0]==batch_size:\n",
        "#             inputs, targets = x.to(device), y.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             output, (sh,sc) = model(inputs, (sh,sc))\n",
        "#             loss = loss_func(output.transpose(1,2),targets)\n",
        "#             sh,sc=sh.detach(),sc.detach()\n",
        "#             loss.backward()\n",
        "#             nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "#             optimizer.step()\n",
        "#             tloss+=loss.item()\n",
        "#         if (i+1)%1000==0:\n",
        "#             print(f\"at epoch {epoch} iteration {i+1}\\\n",
        "#             average loss = {tloss/(i+1)}\")"
      ],
      "metadata": {
        "id": "GBbE9qR0U5wA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "\n",
        "# torch.save(model.state_dict(),\"files/wordLSTM.pth\")\n",
        "# with open(\"files/word_to_int.p\",\"wb\") as fb:\n",
        "#     pickle.dump(word_to_int, fb)"
      ],
      "metadata": {
        "id": "-epakDzYVZnb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate text with the trained LSTM model\n",
        "Generate text by predicting the next token"
      ],
      "metadata": {
        "id": "ILX0FlE5VqNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O wordLSTM.pth \"https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/wordLSTM.pth?ref_type=heads&inline=false\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apim5TXoXgYc",
        "outputId": "a044a297-5b80-4f1a-ac72-d50575d32499"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-03 15:32:08--  https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/wordLSTM.pth?ref_type=heads&inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14723353 (14M) [application/octet-stream]\n",
            "Saving to: ‘wordLSTM.pth’\n",
            "\n",
            "wordLSTM.pth        100%[===================>]  14.04M  21.2MB/s    in 0.7s    \n",
            "\n",
            "2025-02-03 15:32:09 (21.2 MB/s) - ‘wordLSTM.pth’ saved [14723353/14723353]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"wordLSTM.pth\", map_location=device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r76fH2kVucM",
        "outputId": "1cf1aaac-e73b-47e9-f73f-60bfaf9a54ea"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-895e8612c1fe>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"wordLSTM.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sample(model, prompt, length=200):\n",
        "    model.eval()\n",
        "    text = prompt.lower().split(' ')\n",
        "    hc = model.init_hidden(1)\n",
        "    length = length - len(text)\n",
        "    for i in range(0, length):\n",
        "        # if the text length is less than seq_len, use text to predict\n",
        "        if len(text)<=seq_len:\n",
        "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
        "        # otherwise use the last seq_len tokens to predict\n",
        "        else:\n",
        "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])\n",
        "        inputs = x.to(device)\n",
        "        output, hc = model(inputs, hc)\n",
        "        logits = output[0][-1]\n",
        "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
        "        idx = np.random.choice(len(logits), p=p)\n",
        "        text.append(int_to_word[idx])\n",
        "    text=\" \".join(text)\n",
        "    for m in \",.:;?!$()/_&%*@'`\":\n",
        "        text=text.replace(f\" {m}\", f\"{m} \")\n",
        "    text=text.replace('\"  ', '\"')\n",
        "    text=text.replace(\"'  \", \"'\")\n",
        "    text=text.replace('\" ', '\"')\n",
        "    text=text.replace(\"' \", \"'\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "ZtQ82iQsVycq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "print(sample(model, prompt='Anna and the prince'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPhWhHHtV3UH",
        "outputId": "407a8080-7029-43f0-fe8b-e6a1235f0937"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna and the prince did not forget what he had not spoken.  when the softening barrier was not so long as he had talked to his brother,  all the hopelessness of the impression.  \"official tail,  a man who had tried him,  though he had been able to get across his charge and locked close,  and the light round the snow was in the light of the altar villa.  the article in law levin was first more precious than it was to him so that if it was most easy as it would be as the same.  this was now perfectly interested.  when he had got up close out into the sledge,  but it was locked in the light window with their one grass,  and in the band of the leaves of his projects,  and all the same stupid woman,  and really,  and i swung his arms round that thinking of bed.  a little box with the two boys were with the point of a gleam of filling the boy,  noiselessly signed the bottom of his mouth,  and answering them took the red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbS_mUXaV72t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}