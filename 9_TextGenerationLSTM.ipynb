{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGsRKbdy9jlyNF6eTMLd0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/Gen-AI-projects-Pytorch/blob/main/9_TextGenerationLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jgleZ0svM3Q",
        "outputId": "dffacc95-44fb-402f-c125-85f8cb49b3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'i', ',', ' ', 't', 'h', 'e', 'r', 'e', '!']\n"
          ]
        }
      ],
      "source": [
        " # exercise 1\n",
        "text=\"Hi, there!\"\n",
        "tokens=list(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 2: word tokenization\n",
        "text=\"It is unbelievably good!\"\n",
        "text=text.replace(\"!\",\"\")\n",
        "tokens=text.split(\" \")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebPKREENxBY3",
        "outputId": "9a469332-16fc-42f0-ad27-6d5278e40215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'is', 'unbelievably', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 3 :\n",
        "text=\"Hi, there!\"\n",
        "for x in list(\",!\"):\n",
        "    text=text.replace(f\"{x}\",f\" {x}\")\n",
        "tokens=text.split(\" \")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8N_DVlpxJYu",
        "outputId": "104ae574-3c9e-4ea2-ea92-86f14ff7a13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', ',', 'there', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & clean up the text"
      ],
      "metadata": {
        "id": "431Sks85yHpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O anna.txt \"https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/anna.txt?ref_type=heads&inline=false\" # text of anna karenina"
      ],
      "metadata": {
        "id": "rVqTxcr8xmDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b06849-92f0-466b-8429-350d2ac9a354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-03 14:54:00--  https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/anna.txt?ref_type=heads&inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2006029 (1.9M) [application/octet-stream]\n",
            "Saving to: ‘anna.txt’\n",
            "\n",
            "anna.txt            100%[===================>]   1.91M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-02-03 14:54:00 (24.8 MB/s) - ‘anna.txt’ saved [2006029/2006029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./anna.txt\",\"r\") as f:\n",
        "    text=f.read()\n",
        "words=text.split(\" \")\n",
        "print(words[:20])\n",
        "\n",
        "print(set(text.lower())) # unique characters in the text by set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awHgakl9xNo_",
        "outputId": "787e7f33-1a0c-4975-f264-83f720d4f307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\"]\n",
            "{'z', ' ', '0', 'l', '\\n', '.', 'v', 'a', 's', 't', 'u', '?', 'f', \"'\", 'd', ';', ',', '-', 'c', '5', 'm', 'y', 'k', 'o', 'x', 'p', '9', 'q', '8', '2', '1', '_', 'e', 'h', 'g', 'n', ':', 'b', '(', '4', '!', '\"', '3', '6', '`', 'j', 'w', '7', ')', 'r', 'i'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text=text.lower().replace(\"\\n\", \" \")\n",
        "clean_text=clean_text.replace(\"-\", \" \")\n",
        "for x in \",.:;?!$()/_&%*@'`\":\n",
        "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
        "clean_text=clean_text.replace('\"', ' \" ')\n",
        "text=clean_text.split()"
      ],
      "metadata": {
        "id": "_axkEgBUyBkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(text)\n",
        "\n",
        "words=sorted(word_counts, key=word_counts.get, reverse=True) # get unique words\n",
        "print(words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDeygMNxyu9C",
        "outputId": "bb6c6725-f2af-4afa-83e0-f9cd24ad690b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here we will make two dictionaries where the key values ​​are in the same place We will use it further"
      ],
      "metadata": {
        "id": "NkSmWAOCzN38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_length =len(text)\n",
        "num_unique_words =len(words)\n",
        "print(f\"the text contains {text_length} words\")\n",
        "print(f\"there are {num_unique_words} unique tokens\")\n",
        "word_to_int={v:k for k,v in enumerate(words)} # k is number of word repetition and ,v is word\n",
        "int_to_word={k:v for k,v in enumerate(words)}\n",
        "print({k:v for k,v in word_to_int.items() if k in words[:10]})\n",
        "print({k:v for k,v in int_to_word.items() if v in words[:10]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kRaZA_cyzsV",
        "outputId": "ecdfdbf1-a181-419d-acd8-2242ad59b30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the text contains 437098 words\n",
            "there are 12778 unique tokens\n",
            "{',': 0, '.': 1, 'the': 2, '\"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7, \"'\": 8, 'a': 9}\n",
            "{0: ',', 1: '.', 2: 'the', 3: '\"', 4: 'and', 5: 'to', 6: 'of', 7: 'he', 8: \"'\", 9: 'a'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0:20])\n",
        "wordidx=[word_to_int[w] for w in text]\n",
        "print([word_to_int[w] for w in text[0:20]])  # The number of each word is the number of repetitions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFM7oOrGzJv1",
        "outputId": "a0b34724-8fc9-4d50-b605-e77513e61c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
            "[208, 2755, 280, 2981, 83, 31, 2419, 35, 202, 685, 362, 38, 685, 10, 236, 147, 166, 1, 149, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create batches of training data"
      ],
      "metadata": {
        "id": "eshoWUek0fsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "seq_len=100\n",
        "xys=[]\n",
        "for n in range(0, len(wordidx)-seq_len-1):  # 0 : 437098 -100 - 1\n",
        "    x = wordidx[n:n+seq_len]                # x= 0:100   y = 1:101\n",
        "    y = wordidx[n+1:n+seq_len+1]            # x= 1:101   y = 2:102\n",
        "    xys.append((torch.tensor(x),(torch.tensor(y))))"
      ],
      "metadata": {
        "id": "zkgfo1Xz0RDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "batch_size=32\n",
        "loader = DataLoader(xys, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "x,y=next(iter(loader))\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape,y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvhpkC_o0kQ3",
        "outputId": "0356aa0e-ef96-448c-d94f-b8e4561e84ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  39,   31,    2,  ...,  688,  142,    7],\n",
            "        [ 156, 5293,    0,  ...,   38,  330,    0],\n",
            "        [   3,   97,    0,  ...,    0, 1774,   34],\n",
            "        ...,\n",
            "        [  16,  156,    9,  ...,  113,    5,  533],\n",
            "        [   3,    4,   31,  ...,   98,    5,   98],\n",
            "        [ 289,   19,   23,  ...,    9,  828,  550]])\n",
            "tensor([[  31,    2, 2727,  ...,  142,    7,    0],\n",
            "        [5293,    0,   16,  ...,  330,    0,    3],\n",
            "        [  97,    0,    4,  ..., 1774,   34,    3],\n",
            "        ...,\n",
            "        [ 156,    9,  489,  ...,    5,  533,   27],\n",
            "        [   4,   31,   25,  ...,    5,   98,    1],\n",
            "        [  19,   23,    1,  ...,  828,  550,    1]])\n",
            "torch.Size([32, 100]) torch.Size([32, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an LSTM model\n",
        "### LSTM uses token and hidden state to create the next token and hidden state\n",
        "The hidden state stores past information at each step, helping the LSTM understand dependencies between words in a sequence. In every step, a new hidden state is generated and passed to the next step. It works alongside the cell state to maintain both short-term and long-term information."
      ],
      "metadata": {
        "id": "smFyu-i7U9vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, input_size=128, n_embed=128,\n",
        "             n_layers=3, drop_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.input_size=input_size\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_embed = n_embed\n",
        "        vocab_size=len(word_to_int)\n",
        "        self.embedding=nn.Embedding(vocab_size,n_embed)\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
        "            hidden_size=self.n_embed,\n",
        "            num_layers=self.n_layers,\n",
        "            dropout=self.drop_prob,batch_first=True)\n",
        "        self.fc = nn.Linear(input_size, vocab_size) # vocab_size is the number of unique words\n",
        "    def forward(self, x, hc):\n",
        "        embed=self.embedding(x)\n",
        "        x, hc = self.lstm(embed, hc) #hc is tuple of \"hidden state\" and \"cell state\"\n",
        "        x = self.fc(x)\n",
        "        return x, hc\n",
        "    def init_hidden(self, n_seqs):             #Creates two Tensors (hidden state and cell state) with an initial value of zero.\n",
        "        weight = next(self.parameters()).data       #In order that hc is not affected by previous epochs in each epoch, we set it to zero every time\n",
        "        return (weight.new(self.n_layers,n_seqs, self.n_embed).zero_(), #3*32*128\n",
        "                weight.new(self.n_layers,n_seqs, self.n_embed).zero_())"
      ],
      "metadata": {
        "id": "tI-j3T_Z1u50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=WordLSTM().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "g1P3LhH5TRqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6d2ce4-5681-4f98-b637-3ca3da87a50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (embedding): Embedding(12778, 128)\n",
            "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=128, out_features=12778, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "weOEAgePUiiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the LSTM model\n",
        "Because the train takes time, I use the train file"
      ],
      "metadata": {
        "id": "jy8uE2KyVIVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.train()\n",
        "\n",
        "# for epoch in range(50):\n",
        "#     tloss=0\n",
        "#     sh,sc = model.init_hidden(batch_size)\n",
        "#     for i, (x,y) in enumerate(loader):\n",
        "#         if x.shape[0]==batch_size:\n",
        "#             inputs, targets = x.to(device), y.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             output, (sh,sc) = model(inputs, (sh,sc))\n",
        "#             loss = loss_func(output.transpose(1,2),targets)\n",
        "#             sh,sc=sh.detach(),sc.detach()\n",
        "#             loss.backward()\n",
        "#             nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "#             optimizer.step()\n",
        "#             tloss+=loss.item()\n",
        "#         if (i+1)%1000==0:\n",
        "#             print(f\"at epoch {epoch} iteration {i+1}\\\n",
        "#             average loss = {tloss/(i+1)}\")"
      ],
      "metadata": {
        "id": "GBbE9qR0U5wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate text with the trained LSTM model\n",
        "Generate text by predicting the next token.\n",
        "in real we want Continue to generate prpmpt with 200 model tokens"
      ],
      "metadata": {
        "id": "ILX0FlE5VqNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O wordLSTM.pth \"https://gitlab.com/amirbig44/pytorch-gan-course/-/raw/main/wordLSTM.pth?ref_type=heads&inline=false\"\n",
        "# download the model weights file from the link"
      ],
      "metadata": {
        "id": "Apim5TXoXgYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"wordLSTM.pth\", map_location=device))"
      ],
      "metadata": {
        "id": "9r76fH2kVucM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For example, if our prompt is four words, we want the model to predict the next 196 words so that a total of 200 tokens are answered.We also put a condition that if the number of generated words is more than 100 (for example, to generate the 150th word), choose only the last 100."
      ],
      "metadata": {
        "id": "E_hEC15ASHWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sample(model, prompt, length=200):  # length is the number of tokens to generate by the model\n",
        "    model.eval()\n",
        "    text = prompt.lower().split(' ') # text is a list of words prompt\n",
        "    hc = model.init_hidden(1)      # 3*1*128\n",
        "    length = length - len(text)\n",
        "    for i in range(0, length):\n",
        "        # if the text length is less than seq_len=100, use text to predict\n",
        "        if len(text)<=seq_len:  #seq_len = 100\n",
        "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
        "        # otherwise use the last seq_len=100 tokens to predict\n",
        "        else:\n",
        "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])\n",
        "        inputs = x.to(device)\n",
        "        output, hc = model(inputs, hc)\n",
        "        logits = output[0][-1]\n",
        "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy() # p is the probability normalized of each word\n",
        "        idx = np.random.choice(len(logits), p=p)\n",
        "        text.append(int_to_word[idx])\n",
        "    text=\" \".join(text)\n",
        "    for m in \",.:;?!$()/_&%*@'`\":\n",
        "        text=text.replace(f\" {m}\", f\"{m} \")\n",
        "    text=text.replace('\"  ', '\"')\n",
        "    text=text.replace(\"'  \", \"'\")\n",
        "    text=text.replace('\" ', '\"')\n",
        "    text=text.replace(\"' \", \"'\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "ZtQ82iQsVycq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "print(sample(model, prompt='Anna and the prince'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPhWhHHtV3UH",
        "outputId": "407a8080-7029-43f0-fe8b-e6a1235f0937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna and the prince did not forget what he had not spoken.  when the softening barrier was not so long as he had talked to his brother,  all the hopelessness of the impression.  \"official tail,  a man who had tried him,  though he had been able to get across his charge and locked close,  and the light round the snow was in the light of the altar villa.  the article in law levin was first more precious than it was to him so that if it was most easy as it would be as the same.  this was now perfectly interested.  when he had got up close out into the sledge,  but it was locked in the light window with their one grass,  and in the band of the leaves of his projects,  and all the same stupid woman,  and really,  and i swung his arms round that thinking of bed.  a little box with the two boys were with the point of a gleam of filling the boy,  noiselessly signed the bottom of his mouth,  and answering them took the red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbS_mUXaV72t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}